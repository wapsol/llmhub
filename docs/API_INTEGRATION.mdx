# LLMHub API Integration Guide

This guide shows how other software programs can integrate with LLMHub to access multiple LLM providers through a unified API.

## Table of Contents

- [Quick Start](#quick-start)
- [Authentication](#authentication)
- [Available Endpoints](#available-endpoints)
- [Code Examples](#code-examples)
- [Error Handling](#error-handling)
- [Rate Limiting & Billing](#rate-limiting--billing)

## Quick Start

### 1. Get Your API Key

First, obtain an API key from the LLMHub web UI (http://localhost:4000):

1. Navigate to **API Clients** section
2. Click **Create Client**
3. Fill in your application details
4. Copy the generated API key

### 2. Make Your First Request

```bash
curl -X POST http://localhost:4000/api/v1/llm/generate-content \
  -H "X-API-Key: your-api-key-here" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Explain quantum computing in simple terms",
    "provider": "claude",
    "languages": ["en"],
    "max_tokens": 500
  }'
```

## Authentication

All API requests require authentication using an API key passed in the `X-API-Key` header:

```
X-API-Key: your-api-key-here
```

### Security Best Practices

- **Never expose API keys in client-side code**
- Store keys in environment variables or secure vaults
- Rotate keys periodically using the web UI
- Use separate keys for development and production

## Available Endpoints

### Base URL

```
http://localhost:4000/api/v1
```

For production, replace with your deployment URL.

---

### 1. Content Generation

Generate text content using any configured LLM provider.

**Endpoint:** `POST /llm/generate-content`

**Request Body:**

```json
{
  "prompt": "string (required)",
  "provider": "claude | openai | groq (default: claude)",
  "model": "string (optional, provider-specific)",
  "languages": ["en", "de", "fr"] (optional, default: ["en"]),
  "template_id": "uuid (optional)",
  "variables": {} (optional, for templates),
  "max_tokens": 4096 (optional),
  "temperature": 0.7 (optional, 0.0-2.0)
}
```

**Response:**

```json
{
  "content": "Generated text content...",
  "tokens_used": 1234,
  "input_tokens": 100,
  "output_tokens": 1134,
  "cost_usd": 0.045,
  "generation_time_ms": 2500,
  "provider": "claude",
  "model": "claude-3-5-sonnet-20241022"
}
```

---

### 2. Translation

Translate content into multiple languages simultaneously.

**Endpoint:** `POST /llm/translate`

**Request Body:**

```json
{
  "content": {
    "title": "Cloud Security Best Practices",
    "body": "Content to translate...",
    "meta_description": "Optional metadata"
  },
  "source_language": "en",
  "target_languages": ["de", "fr", "it"],
  "provider": "claude",
  "model": "claude-3-5-sonnet-20241022"
}
```

**Response:**

```json
{
  "translations": {
    "de": {
      "title": "Cloud-Sicherheit Best Practices",
      "body": "Translated content...",
      "meta_description": "..."
    },
    "fr": { ... },
    "it": { ... }
  },
  "cost_usd": 0.12,
  "tokens_used": 2500
}
```

---

### 3. Image Generation

Generate images using DALL-E (OpenAI).

**Endpoint:** `POST /llm/generate-image`

**Request Body:**

```json
{
  "prompt": "Modern data center with blue LED lights, professional photo",
  "size": "1024x1024",
  "quality": "standard | hd",
  "n": 1
}
```

**Response:**

```json
{
  "image_url": "https://...",
  "revised_prompt": "A sleek, modern data center...",
  "cost_usd": 0.04
}
```

---

### 4. Content Improvement

Improve or refine existing content.

**Endpoint:** `POST /llm/improve-content`

**Request Body:**

```json
{
  "content": "Original content to improve...",
  "instructions": "Make it more technical and professional",
  "provider": "claude",
  "model": "claude-3-5-sonnet-20241022"
}
```

**Response:**

```json
{
  "improved_content": "Enhanced version...",
  "tokens_used": 800,
  "cost_usd": 0.025
}
```

---

### 5. Prompt Templates

List and use predefined prompt templates.

**Endpoints:**

- `GET /llm/prompts` - List all templates
- `GET /llm/prompts/{template_id}` - Get specific template
- `POST /llm/prompts` - Create custom template (requires permissions)

**Using a Template:**

```json
{
  "template_id": "uuid-of-template",
  "variables": {
    "topic": "Cloud Security",
    "target_audience": "IT Directors",
    "industry": "Healthcare"
  },
  "languages": ["en", "de"]
}
```

---

## Code Examples

### Python

```python
import requests

LLMHUB_URL = "http://localhost:4000/api/v1"
API_KEY = "your-api-key-here"

def generate_content(prompt, provider="claude", max_tokens=1000):
    """Generate content using LLMHub"""
    response = requests.post(
        f"{LLMHUB_URL}/llm/generate-content",
        headers={
            "X-API-Key": API_KEY,
            "Content-Type": "application/json"
        },
        json={
            "prompt": prompt,
            "provider": provider,
            "max_tokens": max_tokens,
            "languages": ["en"]
        }
    )

    if response.status_code == 200:
        data = response.json()
        return data["content"]
    else:
        raise Exception(f"Error: {response.status_code} - {response.text}")

# Usage
content = generate_content("Explain machine learning in simple terms")
print(content)
```

### JavaScript (Node.js)

```javascript
const axios = require('axios');

const LLMHUB_URL = 'http://localhost:4000/api/v1';
const API_KEY = 'your-api-key-here';

async function generateContent(prompt, provider = 'claude') {
  try {
    const response = await axios.post(
      `${LLMHUB_URL}/llm/generate-content`,
      {
        prompt: prompt,
        provider: provider,
        max_tokens: 1000,
        languages: ['en']
      },
      {
        headers: {
          'X-API-Key': API_KEY,
          'Content-Type': 'application/json'
        }
      }
    );

    return response.data.content;
  } catch (error) {
    console.error('Error:', error.response?.data || error.message);
    throw error;
  }
}

// Usage
generateContent('Explain machine learning in simple terms')
  .then(content => console.log(content))
  .catch(error => console.error(error));
```

### cURL

```bash
#!/bin/bash

API_KEY="your-api-key-here"
LLMHUB_URL="http://localhost:4000/api/v1"

# Generate content
curl -X POST "${LLMHUB_URL}/llm/generate-content" \
  -H "X-API-Key: ${API_KEY}" \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Explain machine learning in simple terms",
    "provider": "claude",
    "max_tokens": 1000,
    "languages": ["en"]
  }'

# Translate content
curl -X POST "${LLMHUB_URL}/llm/translate" \
  -H "X-API-Key: ${API_KEY}" \
  -H "Content-Type: application/json" \
  -d '{
    "content": {
      "title": "Machine Learning Guide",
      "body": "Machine learning is..."
    },
    "source_language": "en",
    "target_languages": ["de", "fr"],
    "provider": "claude"
  }'
```

### Go

```go
package main

import (
    "bytes"
    "encoding/json"
    "fmt"
    "io/ioutil"
    "net/http"
)

const (
    LLMHUB_URL = "http://localhost:4000/api/v1"
    API_KEY    = "your-api-key-here"
)

type ContentRequest struct {
    Prompt     string   `json:"prompt"`
    Provider   string   `json:"provider"`
    MaxTokens  int      `json:"max_tokens"`
    Languages  []string `json:"languages"`
}

type ContentResponse struct {
    Content     string  `json:"content"`
    TokensUsed  int     `json:"tokens_used"`
    CostUSD     float64 `json:"cost_usd"`
}

func GenerateContent(prompt string) (string, error) {
    reqBody := ContentRequest{
        Prompt:    prompt,
        Provider:  "claude",
        MaxTokens: 1000,
        Languages: []string{"en"},
    }

    jsonData, err := json.Marshal(reqBody)
    if err != nil {
        return "", err
    }

    req, err := http.NewRequest("POST", LLMHUB_URL+"/llm/generate-content", bytes.NewBuffer(jsonData))
    if err != nil {
        return "", err
    }

    req.Header.Set("X-API-Key", API_KEY)
    req.Header.Set("Content-Type", "application/json")

    client := &http.Client{}
    resp, err := client.Do(req)
    if err != nil {
        return "", err
    }
    defer resp.Body.Close()

    body, err := ioutil.ReadAll(resp.Body)
    if err != nil {
        return "", err
    }

    var result ContentResponse
    err = json.Unmarshal(body, &result)
    if err != nil {
        return "", err
    }

    return result.Content, nil
}

func main() {
    content, err := GenerateContent("Explain machine learning in simple terms")
    if err != nil {
        fmt.Println("Error:", err)
        return
    }
    fmt.Println(content)
}
```

---

## Error Handling

### HTTP Status Codes

- `200` - Success
- `400` - Bad Request (invalid parameters)
- `401` - Unauthorized (invalid or missing API key)
- `403` - Forbidden (inactive API key or budget exceeded)
- `422` - Unprocessable Entity (validation error)
- `429` - Too Many Requests (rate limit exceeded)
- `500` - Internal Server Error
- `503` - Service Unavailable (LLM provider issue)

### Error Response Format

```json
{
  "success": false,
  "error": {
    "code": "INVALID_API_KEY",
    "message": "The provided API key is invalid or inactive",
    "details": { ... }
  }
}
```

### Common Error Codes

- `MISSING_API_KEY` - No X-API-Key header provided
- `INVALID_API_KEY` - API key not found or invalid
- `INACTIVE_CLIENT` - API key is inactive
- `RATE_LIMIT_EXCEEDED` - Too many requests
- `BUDGET_EXCEEDED` - Monthly budget limit reached
- `PROVIDER_ERROR` - LLM provider returned an error
- `VALIDATION_ERROR` - Invalid request parameters

---

## Rate Limiting & Billing

### Rate Limits

Each API client has a rate limit (default: 100 requests/minute). View your limit in the web UI under **API Clients**.

When rate limited, you'll receive a `429` status code with:

```json
{
  "error": {
    "code": "RATE_LIMIT_EXCEEDED",
    "message": "Rate limit exceeded. Try again in 30 seconds.",
    "retry_after": 30
  }
}
```

### Cost Tracking

Every API call returns cost information:

```json
{
  "tokens_used": 1234,
  "input_tokens": 100,
  "output_tokens": 1134,
  "cost_usd": 0.045
}
```

**View detailed billing:**

1. Open LLMHub web UI
2. Navigate to **Billing & Usage**
3. Filter by time range and client

### Budget Limits

Set monthly budget limits per client in the web UI. When exceeded, requests will be rejected with:

```json
{
  "error": {
    "code": "BUDGET_EXCEEDED",
    "message": "Monthly budget limit of $500 exceeded"
  }
}
```

---

## Provider-Specific Notes

### Anthropic Claude

- **Best for:** Complex reasoning, long context (200K tokens)
- **Models:**
  - `claude-3-5-sonnet-20241022` (recommended, balanced)
  - `claude-3-opus-20240229` (most capable, expensive)
  - `claude-3-haiku-20240307` (fast, economical)

### OpenAI

- **Best for:** General tasks, image generation
- **Models:**
  - `gpt-4-turbo-preview` (latest, recommended)
  - `gpt-4` (stable, proven)
  - `gpt-3.5-turbo` (fast, cheap)
- **Image generation:** Use `/llm/generate-image` endpoint

### Groq

- **Best for:** Ultra-fast inference, high throughput
- **Models:**
  - `mixtral-8x7b-32768` (high quality)
  - `llama2-70b-4096` (open-source)
- **Note:** Much cheaper than Claude/OpenAI

---

## Support & Documentation

- **Web UI:** http://localhost:4000
- **API Docs (Swagger):** http://localhost:4000/docs
- **API Docs (ReDoc):** http://localhost:4000/redoc
- **Health Check:** http://localhost:4000/health

For issues or questions, contact your LLMHub administrator.
